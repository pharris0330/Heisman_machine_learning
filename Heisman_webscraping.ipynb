{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://www.sports-reference.com/cfb/awards/heisman-2020.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a soup object from the home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the home page soup for every restaurant\n",
    "\n",
    "Note: Your best bet is to create a list of dictionaries, one for each restaurant. Each dictionary contains the restaurant's name and path from the `href`. The result of your scrape should look something like this:\n",
    "\n",
    "```python\n",
    "restaurants = [\n",
    "    {'name': 'A&W Restaurants', 'href': 'restaurants/1.html'}, \n",
    "    {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', {'id': 'heisman'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = []\n",
    "for row in table.find('tbody').find_all('tr'):\n",
    "    store = {}\n",
    "    store['name'] = row.find('a').text.strip()\n",
    "    store['href'] = row.find('a').attrs['href'].replace('mailto:', '')\n",
    "    restaurants.append(store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the `href`, scrape each restaurant's page and create a single list of food dictionaries.\n",
    "\n",
    "Your list of foods should look something like this:\n",
    "```python\n",
    "foods = [\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Note**: Remove extra white space from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = []\n",
    "restaurants = []\n",
    "\n",
    "for row in table.find('tbody').find_all('tr'):\n",
    "    stuff = {}\n",
    "    stuff['restaurant'] = row.find('a').text.strip()\n",
    "    url = f'https://pages.git.generalassemb.ly/rldaggie/for-scraping/{row.find(\"a\")[\"href\"]}'\n",
    "    res=requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    table2 = soup.find('table')\n",
    "    for row in table2.find('tbody').find_all('tr'):\n",
    "        stuff['calories'] = row.find_all('td')[0].text\n",
    "        stuff['carbs'] = row.find_all('td')[1].text\n",
    "        stuff['category'] = row.find_all('td')[2].text\n",
    "        stuff['fat'] = row.find_all('td')[3].text\n",
    "        stuff['name'] = row.find_all('td')[4].text\n",
    "        foods.append(stuff)\n",
    "            \n",
    "#             url = f'https://pages.git.generalassemb.ly/rldaggie/for-scraping/{row.find(\"a\")[\"href\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'restaurant': 'A&W Restaurants',\n",
       "  'calories': 'A&W® Root Beer',\n",
       "  'carbs': 'Drinks',\n",
       "  'category': '270',\n",
       "  'fat': '0',\n",
       "  'name': '72'},\n",
       " {'restaurant': 'A&W Restaurants',\n",
       "  'calories': 'A&W® Root Beer',\n",
       "  'carbs': 'Drinks',\n",
       "  'category': '270',\n",
       "  'fat': '0',\n",
       "  'name': '72'},\n",
       " {'restaurant': 'A&W Restaurants',\n",
       "  'calories': 'A&W® Root Beer',\n",
       "  'carbs': 'Drinks',\n",
       "  'category': '270',\n",
       "  'fat': '0',\n",
       "  'name': '72'},\n",
       " {'restaurant': 'A&W Restaurants',\n",
       "  'calories': 'A&W® Root Beer',\n",
       "  'carbs': 'Drinks',\n",
       "  'category': '270',\n",
       "  'fat': '0',\n",
       "  'name': '72'},\n",
       " {'restaurant': 'A&W Restaurants',\n",
       "  'calories': 'A&W® Root Beer',\n",
       "  'carbs': 'Drinks',\n",
       "  'category': '270',\n",
       "  'fat': '0',\n",
       "  'name': '72'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foods[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a pandas DataFrame from your list of foods\n",
    "\n",
    "**Note**: Your DataFrame should have 5,131 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(foods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('./datasets/foods.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
